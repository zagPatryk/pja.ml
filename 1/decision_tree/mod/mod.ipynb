{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with max fitted depth of 5:\n",
      "root ::: Split at variable 2 at 2.5999999999999996\n",
      "  right ::: Split at variable 3 at 1.65 ([37 44])\n",
      "   right.right ::: Split at variable 2 at 4.85 ([ 1 40])\n",
      "    right.right.right ([37] and the leaf value is [2])\n",
      "    right.right.left ::: Split at variable 1 at 3.1 ([1 3])\n",
      "     right.right.left.right ([1] and the leaf value is [1])\n",
      "     right.right.left.left ([3] and the leaf value is [2])\n",
      "   right.left ::: Split at variable 2 at 4.95 ([36  4])\n",
      "    right.left.right ([1 4] and the leaf value is None)\n",
      "     right.left.right.right ([3] and the leaf value is [2])\n",
      "     right.left.right.left ::: Split at variable 1 at 2.45 ([1 1])\n",
      "      right.left.right.left.right ([1] and the leaf value is [1])\n",
      "      right.left.right.left.left ([1] and the leaf value is [2])\n",
      "    right.left.left ([35] and the leaf value is [1])\n",
      "  left ([39] and the leaf value is [0])\n",
      "\n",
      "Rough test: Prediction=0, True Value=0\n"
     ]
    }
   ],
   "source": [
    "# Modified by Pz\n",
    "# Author: Dominik Deja\n",
    "# First created: 20.06.2021\n",
    "# This is a stack-based implementation of a decision tree\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import Any, NoReturn\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def attrgetter(obj: object, name: str, value: Any = None) -> Any:\n",
    "    \"\"\"\n",
    "    Returns value from nested objects/chained attributes (basically, getattr() on steroids)\n",
    "    :param obj: Primary object\n",
    "    :param name: Path to an attribute (dot separated)\n",
    "    :param value: Default value returned if a function fails to find the requested attribute value\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for attribute in name.split('.'):\n",
    "        obj = getattr(obj, attribute, value)\n",
    "    return obj\n",
    "\n",
    "\n",
    "def attrsetter(obj: object, name: str, value: Any) -> NoReturn:\n",
    "    \"\"\"\n",
    "    Sets the value of an attribute of a (nested) object (basically, setattr() on steroids)\n",
    "    :param obj: Primary object\n",
    "    :param name: Path to an attribute (dot separated)\n",
    "    :param value: Value to be set\n",
    "    \"\"\"\n",
    "    pre, _, post = name.rpartition('.')\n",
    "    setattr(attrgetter(obj, pre) if pre else obj, post, value)\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Classification decision tree\n",
    "    Data must be provided on instance creation, then fit() can be used to fit the tree to the data and\n",
    "    predict() to predict classes of the new data samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth: int = 3) -> NoReturn:\n",
    "        \"\"\"\n",
    "        :param max_depth: maximum depth of a tree\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.root = Node()\n",
    "        self.fitted_depth = 0\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        if self.fitted_depth == 0:\n",
    "            return 'This tree is still a sapling. There\\'s nothing to show'\n",
    "        else:\n",
    "            s = f'Tree with max fitted depth of {self.fitted_depth}:\\n'\n",
    "            s += f'root ::: Split at variable {self.root.variable} at {self.root.threshold}\\n'\n",
    "            stack = ['left', 'right']\n",
    "            while stack:\n",
    "                name = stack.pop()\n",
    "                curr_depth = name.count('.') + 2\n",
    "                if attrgetter(self.root, f'{name}.variable'):\n",
    "                    s += f\"{' ' * curr_depth}{name} ::: Split at variable {attrgetter(self.root, f'{name}.variable')} at \" \\\n",
    "                         f\"{attrgetter(self.root, f'{name}.threshold')} (\" \\\n",
    "                         f\"{np.unique(attrgetter(self.root, f'{name}.target'), return_counts=True)[1]})\\n\"\n",
    "                else:\n",
    "                    s += f\"{' ' * curr_depth}{name} ({np.unique(attrgetter(self.root, f'{name}.target'), return_counts=True)[1]}\" \\\n",
    "                         f\" and the leaf value is {attrgetter(self.root, f'{name}.leaf_value')})\\n\"\n",
    "                if attrgetter(self.root, f'{name}.left'):\n",
    "                    stack.append(f'{name}.left')\n",
    "                if attrgetter(self.root, f'{name}.right'):\n",
    "                    stack.append(f'{name}.right')\n",
    "            return s\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(x: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Entropy, as defined in information theory (https://en.wikipedia.org/wiki/Entropy_(information_theory))\n",
    "        :param x: vector of real values\n",
    "        :return: entropy of a vector x\n",
    "        \"\"\"\n",
    "        if x.size == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            counts = np.unique(x, return_counts=True)[1]\n",
    "            norm_counts = counts / counts.sum()\n",
    "            return -(norm_counts * np.log(norm_counts)).sum()\n",
    "\n",
    "    def information_gain(self, parent: np.ndarray, left_child: np.ndarray, right_child: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Information gain, as defined on Wikipedia (https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)\n",
    "        :param parent: float vector\n",
    "        :param left_child: float vector\n",
    "        :param right_child: float vector\n",
    "        :return: float denoting information gain for a given split (parent into its children)\n",
    "        \"\"\"\n",
    "        return self.entropy(parent) - (left_child.size / parent.size * self.entropy(left_child) +\n",
    "                                       right_child.size / parent.size * self.entropy(right_child))\n",
    "\n",
    "    @staticmethod\n",
    "    def moving_average(x: np.ndarray, w: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Moving average of vector x with w-wide window\n",
    "        :param x: float vector\n",
    "        :param w: width of the moving window\n",
    "        :return: float vector with averaged values\n",
    "        \"\"\"\n",
    "        return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "    def find_best_split(self, data: np.ndarray, target: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Searches for the best split on a given data with respect to the dependent variable (using information gain criterion)\n",
    "        :param data: NxM (where N denotes #observations and M denotes #variables) numpy array containing independent variables\n",
    "        :param target: numpy vector containing dependent variable\n",
    "        :return: dictionary with best split variable, threshold and gain\n",
    "        \"\"\"\n",
    "        best_split = {'variable': None,\n",
    "                      'threshold': None,\n",
    "                      'gain': -1}\n",
    "        if np.unique(target).size == 1:\n",
    "            return best_split\n",
    "        for variable in range(data.shape[1]):\n",
    "            indices = data[:, variable].argsort()\n",
    "            # Threshold is set to be a point in between two values (in a monotonically increasing set of unique values)\n",
    "            thresholds = self.moving_average(data[indices, variable], 2)\n",
    "            for threshold in thresholds:\n",
    "                left_indices = data[:,\n",
    "                               variable] < threshold  # TODO: Clean it, if possible, as it adds unnecessary complexity\n",
    "                gain = self.information_gain(target, target[left_indices], target[np.invert(left_indices)])\n",
    "                if gain > best_split['gain']:\n",
    "                    best_split['variable'] = variable\n",
    "                    best_split['threshold'] = threshold\n",
    "                    best_split['gain'] = gain\n",
    "        return best_split\n",
    "\n",
    "    def fit(self, data: np.ndarray = None, target: np.ndarray = None) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Grows a binary classification tree using greedy approach and information gain criterion\n",
    "        :param data: NxM (where N denotes #observations and M denotes #variables) numpy array containing independent variables\n",
    "        :param target: numpy vector containing dependent variable\n",
    "        \"\"\"\n",
    "        best_split = self.find_best_split(data, target)\n",
    "        left_indices = data[:, best_split['variable']] < best_split['threshold']\n",
    "        self.root.variable = best_split['variable']\n",
    "        self.root.threshold = best_split['threshold']\n",
    "\n",
    "        self.fitted_depth = 1\n",
    "\n",
    "        left = Node(name='left',\n",
    "                    data=data[left_indices, :],\n",
    "                    target=target[left_indices],\n",
    "                    curr_depth=1)\n",
    "\n",
    "        attrsetter(self.root, 'left', left)\n",
    "        left.grow_tree(self)\n",
    "\n",
    "        right = Node(name='right',\n",
    "                     data=data[np.invert(left_indices), :],\n",
    "                     target=target[np.invert(left_indices)],\n",
    "                     curr_depth=1)\n",
    "\n",
    "        attrsetter(self.root, 'right', right)\n",
    "        right.grow_tree(self)\n",
    "\n",
    "    def get_prediction(self, x: np.ndarray, name: str = '') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns predicted class(es) for a given observation (numpy vector)\n",
    "        :param x: float vector\n",
    "        :param name: path to an attribute (dot separated)\n",
    "        :return: vector with predicted class(es)\n",
    "        \"\"\"\n",
    "        if attrgetter(self.root, f'{name}.leaf_value') is not None:\n",
    "            return attrgetter(self.root, f'{name}.leaf_value')[0]\n",
    "        if name == '':  # TODO: Check if it's possible to get rid of default case\n",
    "            if x[attrgetter(self.root, f'variable')] < attrgetter(self.root, f'threshold'):\n",
    "                return self.get_prediction(x, name='left')\n",
    "            else:\n",
    "                return self.get_prediction(x, name='right')\n",
    "        else:\n",
    "            if x[attrgetter(self.root, f'{name}.variable')] < attrgetter(self.root, f'{name}.threshold'):\n",
    "                return self.get_prediction(x, name=f'{name}.left')\n",
    "            else:\n",
    "                return self.get_prediction(x, name=f'{name}.right')\n",
    "\n",
    "    def predict(self, new_data: np.ndarray) -> list:\n",
    "        \"\"\"\n",
    "        Returns predicted classes for given observations\n",
    "        :param new_data: NxM (where N denotes #observations and M denotes #variables) numpy array\n",
    "        :return: list with predicted classes\n",
    "        \"\"\"\n",
    "        return [self.get_prediction(x) for x in new_data]\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Building block of each tree\n",
    "    May contain children nodes (left and right) or be a final node (called \"leaf\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name=None, data: np.ndarray = None, target: np.ndarray = None, left: Node = None,\n",
    "                 right: Node = None,\n",
    "                 curr_depth: int = None, variable: int = None, threshold: float = None, leaf_value: np.ndarray = None,\n",
    "                 ) -> NoReturn:\n",
    "        \"\"\"\n",
    "        :param data: NxM (where N denotes #observations and M denotes #variables) numpy array containing independent variables\n",
    "        :param target: numpy vector containing dependent variable\n",
    "        :param left: (if exists) node containing observations smaller than a given threshold at a given variable\n",
    "        :param right: (if exists) node containing observations bigger than a given threshold at a given variable\n",
    "        :param curr_depth: number of parent nodes directly above the current node\n",
    "        :param variable: variable used to split data\n",
    "        :param threshold: threshold at which data was split\n",
    "        :param leaf_value: (if a node is a leaf, i.e. a final node with no children) the most frequent value(s) of a\n",
    "                           dependent variable in a given node\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.curr_depth = curr_depth\n",
    "        self.variable = variable\n",
    "        self.threshold = threshold\n",
    "        self.leaf_value = leaf_value\n",
    "        self.name = name\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f'This node is at level: {self.curr_depth}'\n",
    "\n",
    "    def grow_tree(self, tree):\n",
    "        name = self.name\n",
    "        curr_depth = name.count('.') + 2\n",
    "        data = attrgetter(tree.root, f'{name}.data')\n",
    "        target = attrgetter(tree.root, f'{name}.target')\n",
    "\n",
    "        best_split = tree.find_best_split(data=data, target=target)\n",
    "        if curr_depth <= tree.max_depth and np.unique(target).size > 1 and best_split['gain'] > 0:\n",
    "            if tree.fitted_depth < curr_depth:\n",
    "                tree.fitted_depth = curr_depth\n",
    "            left_indices = data[:, best_split['variable']] <= best_split['threshold']\n",
    "            attrsetter(tree.root, f'{name}.variable', best_split['variable'])\n",
    "            attrsetter(tree.root, f'{name}.threshold', best_split['threshold'])\n",
    "\n",
    "            left = Node(name=f'{name}.left',\n",
    "                        data=data[left_indices, :],\n",
    "                        target=target[left_indices],\n",
    "                        curr_depth=curr_depth)\n",
    "\n",
    "            attrsetter(tree.root, f'{name}.left', left)\n",
    "\n",
    "            right = Node(name=f'{name}.right',\n",
    "                         data=data[np.invert(left_indices), :],\n",
    "                         target=target[np.invert(left_indices)],\n",
    "                         curr_depth=curr_depth)\n",
    "\n",
    "            attrsetter(tree.root, f'{name}.right', right)\n",
    "\n",
    "            left.grow_tree(tree)\n",
    "            right.grow_tree(tree)\n",
    "        else:\n",
    "            target_values, target_counts = np.unique(target, return_counts=True)\n",
    "            attrsetter(tree.root, f'{name}.leaf_value', target_values[target_counts == target_counts.max()])\n",
    "\n",
    "def main():\n",
    "    iris = datasets.load_iris()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=1)\n",
    "    tree = DecisionTree(max_depth=6)\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(tree)\n",
    "    print(f'Rough test: Prediction={tree.get_prediction(X_test[0, :])}, True Value={y_test[0]}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
