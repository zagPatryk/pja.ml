{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "The data has been split into two groups:\n",
    "-training set (train.csv)\n",
    "-test set (test.csv)\n",
    "\n",
    "**The training set** should be used to build your machine learning models. For the training set, we provide the outcome (also known as the “ground truth”) for each passenger. Your model will be based on “features” like passengers’ gender and class. You can also use feature engineering to create new features.\n",
    "\n",
    "**The test set** should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n",
    "\n",
    "We also include **gender_submission.csv**, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "| Variable |                 Definition                 |                       Key                      |\n",
    "|:--------:|:------------------------------------------:|:----------------------------------------------:|\n",
    "| survival | Survival                                   | 0 = No, 1 = Yes                                |\n",
    "| pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n",
    "| sex      | Sex                                        |                                                |\n",
    "| Age      | Age in years                               |                                                |\n",
    "| sibsp    | # of siblings / spouses aboard the Titanic |                                                |\n",
    "| parch    | # of parents / children aboard the Titanic |                                                |\n",
    "| ticket   | Ticket number                              |                                                |\n",
    "| fare     | Passenger fare                             |                                                |\n",
    "| cabin    | Cabin number                               |                                                |\n",
    "| embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "## Variable Notes\n",
    "\n",
    "**pclass**: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "**sibsp**: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancés were ignored)\n",
    "\n",
    "**parch**: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "\n",
    "## Link\n",
    "\n",
    "Topic:\n",
    "[Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n",
    "\n",
    "Helpers:\n",
    "[YT random forest](https://www.youtube.com/watch?v=fATVVQfFyU0)\n",
    "[Kaggle tutorial](https://www.kaggle.com/code/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)\n",
    "[Kaggle plots examples](https://www.kaggle.com/code/andrej0marinchenko/titanic-machine-learning-from-disaster/)\n",
    "[Udemy with love](https://www.udemy.com/course/machinelearning/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# do operacji na tablicach\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# do splitera danych na zbiór testowy i walidacyjny\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# do obróbki wykresów\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# do rysowania heatmapy\n",
    "import seaborn as sns\n",
    "\n",
    "# to do podmiany NaN na coś innego\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# do to rozdzielenia właściwości na inne kolumn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# do pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "# żeby był scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# do random forestu\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# do cross validacji\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# do confusion matrix żęby zwizualizować wyniki ze zbioru testowego\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# do decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# do narysowania drzewa decyzyjnego\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# do logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# do XGBoost\n",
    "from xgboost import XGBClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titanic_train = pd.read_csv(\"train.csv\")\n",
    "titanic_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titanic_train.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stworzenie heat mapy prezentującej korelacje między właściwościami\n",
    "ax = plt.subplots(figsize =(14, 12))\n",
    "cp = sns.color_palette(\"icefire\", 18)\n",
    "ax = sns.heatmap(titanic_train.corr(), cmap=cp, linecolor=\"white\", linewidths=0.2, annot=True)\n",
    "ax.hlines([1,2], *ax.get_xlim(), color= \"green\", linewidth=3)\n",
    "ax.vlines([1,2], *ax.get_ylim(), color= \"green\", linewidth=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Podział danych\n",
    "\n",
    "Prawdopodobnie nie potrzebne w tym momencie - tylko informacyjnie i 'na zapas'\n",
    "Wszystkie dane z niego są pominięte"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tworze spliter, którym podziele dane\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "# n_splits - Number of re-shuffling & splitting iterations.\n",
    "# test_size - represent the proportion of the dataset to include in the test split."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# spliter zdefiniowany jako klasa żeby tego wszystkiego nie powtarzać pare razy w kolejnej komórce\n",
    "class Spliter(StratifiedShuffleSplit):\n",
    "\n",
    "    def splitBy(columns):\n",
    "        print(columns)\n",
    "        # dzieli dane tak aby proporcje drugiej wartości były równe pomiędzy oba setami\n",
    "        # po to by uniknać sytuacji (teoretycznie możliwej) że w train setcie będą tylko wartości że ktoś przeżył\n",
    "        for train_index, test_index  in split.split(titanic_train, titanic_train[columns]):\n",
    "            spl_train_set = titanic_train.loc[train_index]\n",
    "            spl_test_set  = titanic_train.loc[test_index]\n",
    "        return spl_train_set, spl_test_set\n",
    "\n",
    "\n",
    "    # licze stosunek wartości umarł/przeżył między setami żeby wypisać żeby udowodnić że dzielenie przez konkretą kolumne ma sens\n",
    "    def countRatio(spl_train_set, spl_test_set):\n",
    "        c_spl_train_set = spl_train_set.groupby([\"Survived\"])[\"Survived\"].count()\n",
    "        c_spl_test_set = spl_test_set.groupby([\"Survived\"])[\"Survived\"].count()\n",
    "        train_ratio = c_spl_train_set[0] / c_spl_train_set[1]\n",
    "        test_ratio = c_spl_test_set[0] / c_spl_test_set[1]\n",
    "        print(\"Train ratio: \" + str(train_ratio))\n",
    "        print(\"Test ratio: \" + str(test_ratio))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wypisuje na ekran stosunki przy różnych podziałach - jako dowód że spliter miał sens\n",
    "spl_train_set, spl_test_set = Spliter.splitBy([\"Pclass\"])\n",
    "Spliter.countRatio(spl_train_set, spl_test_set)\n",
    "\n",
    "spl_train_set, spl_test_set = Spliter.splitBy([\"Pclass\",\"Survived\"])\n",
    "Spliter.countRatio(spl_train_set, spl_test_set)\n",
    "\n",
    "spl_train_set, spl_test_set = Spliter.splitBy([\"Survived\"])\n",
    "Spliter.countRatio(spl_train_set, spl_test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Obróbka danych"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# musimy usupełnić puste dane w kolumnach np NaN w Age\n",
    "# robimy to teraz a nie przed podziałem bo nie chcemy wpływać na dane zanim będą rozdzielone bo możemy wpłynąć na wynik końcowy\n",
    "# powiedzmy że wstawiamy średni wiek do wszystkich kolumn gdzie jest NaN a średni wiek będzie inny w w zależności od tego kiedy dokonaliśmy podziału"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zamiast ręcznie wszystko wywoływać za każdym razem tworzymy pipeline tzn taki ciąg zdarzeń\n",
    "# zdefiniujemy pare klas, które będą modyfikować dataSet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AgeImputer(TransformerMixin):\n",
    "\n",
    "    # tej metody wymaga pipeline\n",
    "    def fit(self, dataFrame):\n",
    "        return self\n",
    "\n",
    "    # ta metoda tak nazwana bo pipeline ją wywołuje\n",
    "    def transform(self, dataFrame):\n",
    "        # imputer który uzupełni dane średnią wszystkich innych wartości\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        dataFrame['Age'] = imputer.fit_transform(dataFrame[['Age']])\n",
    "        return dataFrame\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kolumny tekstowe muszą być enkodowane bo w ML nie używamy tekstu tylko liczby\n",
    "# więc musimy np C -> 0, S -> 1\n",
    "# to teoretycznie by wystarczyło ale może prowadzić do błędnych predykcji\n",
    "# model może szukać zależności między wierszami i jak zobaczyłby pattern to może wpłynąć na jego predykcje\n",
    "# np jakby wiersze były by w kolejności żę by wychodziło 0 1 2 0 1 2 0 1 2 to może to powiązać z passangerId i prowadzić do złych predykcji\n",
    "# dlatego wartości C S itd rozdzielamy do osobnych kolumn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FeatureEncoder(TransformerMixin) :\n",
    "\n",
    "    def fit(self, dataFrame):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataFrame):\n",
    "        encoder = OneHotEncoder()\n",
    "        dataFrame = self.transformEmbarked(encoder, dataFrame)\n",
    "        dataFrame = self.transformAge(encoder, dataFrame)\n",
    "        return dataFrame\n",
    "\n",
    "    def transformEmbarked(self, encoder, dataFrame):\n",
    "        # fituje encoder z kolumną danych\n",
    "        matrix = encoder.fit_transform(dataFrame[['Embarked']]).toarray()\n",
    "\n",
    "        column_names = dataFrame['Embarked'].unique()\n",
    "        # print(column_names)\n",
    "\n",
    "        # tym .T obracamy tabele (zamiana x z y) żeby łatwo wymieniać wartości\n",
    "        for i in range(len(matrix.T)):\n",
    "            dataFrame[column_names[i]] = matrix.T[i]\n",
    "        return dataFrame\n",
    "\n",
    "    def transformAge(self, encoder, dataFrame):\n",
    "        matrix = encoder.fit_transform(dataFrame[['Sex']]).toarray()\n",
    "\n",
    "        column_names = dataFrame['Sex'].unique()\n",
    "\n",
    "        for i in range(len(matrix.T)):\n",
    "            dataFrame[column_names[i]] = matrix.T[i]\n",
    "\n",
    "        return dataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ColumnsDropper(TransformerMixin):\n",
    "\n",
    "    def fit(self, dataFrame):\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataFrame):\n",
    "        print(dataFrame)\n",
    "\n",
    "        # zostawiam:\n",
    "            # PassengerId - żeby wiedzieć kto jest kim\n",
    "            # Survived - czy umarł czy nie\n",
    "            # Pclass - im lepsza klasa tym pewnie miał bliżej do szalup i go lepiej traktowano\n",
    "            # SibSp, Parch - bo może trzymali się razem albo ratowali się wzajemnie\n",
    "            # rozdzielone Sex - bo kobiety i dzieci przodem\n",
    "            # rozdzielone Embarked - im później wsiedli tym pewnie mięli kabiny bliżej wyjścia\n",
    "\n",
    "        # usuwam:\n",
    "            # Ticket - bo randomowa nazwa\n",
    "            # Name - nie zauważono korelacji imienia ze śmiertelnością\n",
    "            # Cabin - brakuje mega wiele danych tutaj - titanic był podzielone na strefy i w nazwie kabiny jest strefa, mając pełne dane i wyciągając literę z nazwy model byłby dokładniejszy ale brakuje większość danych\n",
    "            # Embarked - rozdzielamy to na osobne kolumny\n",
    "            # Sex - rozdzielamy to na osobne kolumny\n",
    "            # Fare - opłata za bilet wpływa na klasę ale nie wpływa na umieralność\n",
    "            # np.nan - podczas użycia dataFrame['Embarked'].unique() jeżeli istniały kolumny puste tzn z NaN to zostały dodane kolumny z nazwą \"NaN\". To nie jest nazwa testowa tylko taka 'pusta'\n",
    "            # nie możemy jej usunać wpisująć jej nazwę \"NaN\" tylko musimy dosłownie powiedzieć że jest to puste wiec np.nan\n",
    "        # może się okazać, że w ogóle nie było jakieś tej kolumny to olewamy errors=ignore\n",
    "        return dataFrame.drop([\"Ticket\", \"Name\", \"Cabin\", \"Embarked\", \"Sex\", np.nan, \"Fare\"], axis=1, errors=\"ignore\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tworzymy pipeline przez który mozna przepuścić dane i zostaną obropione po kolei przez każdą klasę\n",
    "pipeline = Pipeline([(\"AgeImputer\", AgeImputer()),\n",
    "                     (\"FeatureEncoder\", FeatureEncoder()),\n",
    "                     (\"ColumnsDropper\", ColumnsDropper())])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "titanic_train.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transofmracja danych treningowych\n",
    "\n",
    "transformed_train_set = pipeline.fit_transform(titanic_train)\n",
    "transformed_train_set.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = plt.subplots(figsize =(14, 12))\n",
    "ax = sns.heatmap(transformed_train_set.corr(), cmap=cp, linecolor=\"white\", linewidths=0.2)\n",
    "ax.hlines([1, 2], *ax.get_xlim(), color=\"green\", linewidth=3)\n",
    "ax.vlines([1, 2], *ax.get_ylim(), color=\"green\", linewidth=3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transofmracja danych testowych\n",
    "# tutaj żeby rozdzielić 'moduły' dokumentu\n",
    "\n",
    "titanic_test = pd.read_csv(\"test.csv\")\n",
    "transformed_test_set = pipeline.fit_transform(titanic_test)\n",
    "# fillna wypełnia NA/NaN wartościami według metody: ffill -> propaguj ostatnią ważną obserwację do następnego ważnego uzupełnienia\n",
    "\n",
    "transformed_test_set = transformed_test_set.fillna(method=\"ffill\")\n",
    "transformed_test_set.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standaryzacja danych do modeli machine learning polega na przekształceniu danych pierwotnych, aby ich rozkład miał średnią wartość równą 0 i odchylenie standardowe równe 1. Od każdej wartość w kolumnie danych będzie odejmowana średnia wartość kolumny, a następnie to co wyjdzie będzie podzielona przez odchylenie standardowe kolumny danych. Opisany proces dotyczy każdej kolumny oddzielnie.\n",
    "# http://sigmaquality.pl/uncategorized/standaryzacja-danych-do-modeli-machine-learning-za-pomoca-standardscaler/\n",
    "# w skrócie zmienia zwykłe dane w -1.51772497e+00  8.20947424e-01 -6.31268373e-01"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ScalerSpliter():\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    def fitAndSplit(self, dataFrame):\n",
    "        # dzielimy trainset -> właściwości / wyniki\n",
    "        X = dataFrame.drop(['Survived'], axis=1)\n",
    "        Y = dataFrame['Survived']\n",
    "\n",
    "        # dopasowywujemy fit'er do danych\n",
    "        X_data = self.scaler.fit_transform(X)\n",
    "        # musimy to przekonwertować do numpy arraya\n",
    "        Y_data = Y.to_numpy()\n",
    "\n",
    "        return X_data, Y_data\n",
    "\n",
    "    def transform(self, dataFrame) :\n",
    "        return self.scaler.transform(dataFrame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# stworze tutaj obiekt raz i będę go używać też w modelu testowym\n",
    "# scaler fitujemy do traningowego setu i wykorzystujemy go później w testowym - przez wszystkie uśrednienia i dopasowania\n",
    "spliter = ScalerSpliter()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Metody predykcji"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random forest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_forest, Y_train_forest = spliter.fitAndSplit(transformed_train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# w zależności od hiperparametrów model będzie miał różne wyniki\n",
    "# to pomoże nam zdecydować, jakie parametry są najlepsze\n",
    "class EstimatorFinder:\n",
    "    def getBestEstimator(X_train, Y_train):\n",
    "        clf = RandomForestClassifier()\n",
    "\n",
    "        # parametry random forestu\n",
    "        # będą wypróbowane wszystkie w różnych kombinacjach np 10 None 2 / 10 None 3 / 10 None 4 itd\n",
    "        param_grid = [\n",
    "            {\"n_estimators\": [10, 100, 200, 500],\"max_depth\": [None, 5, 10], \"min_samples_split\": [2,3,4]}\n",
    "        ]\n",
    "\n",
    "        # cross validation tzn program dzieli dataSet na np 10 części\n",
    "        # 9 częściami trenuje dane, 1 testuje\n",
    "        # przerabiam wszystkie kombinacje tak żeby każdy 'fold'/część była użyta raz jako walidacja\n",
    "        # i tak GridSearchCV znajduje optymalną kombinacje hiperparametrów\n",
    "        # parametr - cv to ile foldów\n",
    "        grid_search = GridSearchCV(clf, param_grid, cv=3, scoring=\"accuracy\", return_train_score=True)\n",
    "        grid_search.fit(X_train, Y_train)\n",
    "        return grid_search.best_estimator_\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest_clf = EstimatorFinder.getBestEstimator(X_train_forest, Y_train_forest)\n",
    "forest_clf"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# teraz sprawdzenie na danych testowych jaka jest dokładność"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_forest = spliter.transform(transformed_test_set)\n",
    "forest_predictions = forest_clf.predict(X_test_forest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# otrzymamy tylko predykcje tak więc musimy 'odtworzyć' pełną tabele\n",
    "final_forest_table = pd.DataFrame(titanic_test['PassengerId'])\n",
    "final_forest_table['Survived'] = forest_predictions\n",
    "final_forest_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zapis do pliku <- i to wysyłamy do kaggla\n",
    "final_forest_table.to_csv(\"random_forest.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trochę statystyk do zbioru testowego na podstawie gotowego random forestu"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "forest_train_set_predictions = forest_clf.predict(X_train_forest)\n",
    "\n",
    "# confusion matrix na podstawie danych testowych\n",
    "confusion_matrix(Y_train_forest, forest_train_set_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# wynik modelu - podliczone to co w confusion matrixie\n",
    "forest_clf.score(X_train_forest, Y_train_forest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Decision tree"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_tree, Y_train_tree = spliter.fitAndSplit(transformed_train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth = 10, random_state = 0)\n",
    "tree_clf.fit(X_train_tree, Y_train_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_tree = spliter.transform(transformed_test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_predictions = tree_clf.predict(X_test_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_tree_table = pd.DataFrame(titanic_test['PassengerId'])\n",
    "final_tree_table['Survived'] = tree_predictions\n",
    "final_tree_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 16))\n",
    "plot_tree(tree_clf, filled=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zapis do pliku <- i to wysyłamy do kaggla\n",
    "final_forest_table.to_csv(\"tree.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trochę statystyk do zbioru testowego na podstawie gotowego drzewa decyzyjnego"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_predictions_train = tree_clf.predict(X_train_tree)\n",
    "confusion_matrix(Y_train_tree, tree_predictions_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tree_clf.score(X_train_tree, Y_train_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_log, Y_train_log = spliter.fitAndSplit(transformed_train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(random_state=0, max_iter=1000)\n",
    "log_clf.fit(X_train_log, Y_train_log)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_log = spliter.transform(transformed_test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_predictions = log_clf.predict(X_test_tree)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_log_table = pd.DataFrame(titanic_test['PassengerId'])\n",
    "final_log_table['Survived'] = log_predictions\n",
    "final_log_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zapis do pliku <- i to wysyłamy do kaggla\n",
    "final_log_table.to_csv(\"reg_log.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trochę statystyk do zbioru testowego na podstawie gotowego logistic regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_predictions_train = log_clf.predict(X_train_tree)\n",
    "confusion_matrix(Y_train_log, log_predictions_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_clf.score(X_train_log, Y_train_log)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGB Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_xgb, Y_train_xgb = spliter.fitAndSplit(transformed_train_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_train_xgb, Y_train_xgb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test_xgb = spliter.transform(transformed_test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_clf.predict(X_test_xgb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_xgb_table = pd.DataFrame(titanic_test['PassengerId'])\n",
    "final_xgb_table['Survived'] = xgb_predictions\n",
    "final_xgb_table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# zapis do pliku <- i to wysyłamy do kaggla\n",
    "final_xgb_table.to_csv(\"xgb.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trochę statystyk do zbioru testowego na podstawie gotowego xbt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_clf.score(X_train_xgb, Y_train_xgb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xgb_predictions = xgb_clf.predict(X_train_xgb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "confusion_matrix(Y_train_xgb, xgb_predictions)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
